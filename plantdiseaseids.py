# -*- coding: utf-8 -*-
"""PlantDiseaseIDs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQqT6lpyW0uFySIPmT9RgQSNQvtXSoqO

## Plant Disease Dataset Profililng

### Part 0: Initialize & Read in Data
"""

import kagglehub
import os
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import matplotlib.colors as mcolors
import keras
from keras import layers
from tensorflow import data as tf_data

# Download latest version - code pulled from kaggle website: https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset

path = kagglehub.dataset_download("vipoooool/new-plant-diseases-dataset")

print("Path to dataset files:", path)

# List the contents of the downloaded dataset directory
dataset_path = path # 'path' variable holds the dataset directory from the previous step
print(f"Contents of {dataset_path}:")
for item in os.listdir(dataset_path):
    print(item)

base_dataset_path = dataset_path # Re-using the 'dataset_path' variable

# Define the paths to the actual data directories
actual_data_paths = {
    'Augmented Dataset': os.path.join(base_dataset_path, 'New Plant Diseases Dataset(Augmented)', 'New Plant Diseases Dataset(Augmented)'),
    'Test Dataset': os.path.join(base_dataset_path, 'test', 'test')
}

print("Exploring actual data directories and counting contents:\n")

for dataset_type, folder_path in actual_data_paths.items():
    print(f"--- {dataset_type} ---")
    print(f"Path: {folder_path}")

    subfolders = []
    files = []

    if os.path.exists(folder_path):
        try:
            for item in os.listdir(folder_path):
                item_path = os.path.join(folder_path, item)
                if os.path.isdir(item_path):
                    subfolders.append(item)
                else:
                    files.append(item)

            print(f"  Number of Subfolders: {len(subfolders)}")
            print(f"  Number of Files: {len(files)}")
            print("  Subfolders (first 5):")
            for i, sub in enumerate(subfolders[:5]):
                print(f"    - {sub}")
            if len(subfolders) > 5:
                print(f"    ...and {len(subfolders) - 5} more")

            print("  Files (if any, first 5):")
            if files:
                for i, file in enumerate(files[:5]):
                    print(f"    - {file}")
                if len(files) > 5:
                    print(f"    ...and {len(files) - 5} more")
            else:
                print("    No immediate files found.")

        except Exception as e:
            print(f"  An error occurred while listing contents: {e}")
    else:
        print("  Directory not found.")
    print("\n")

"""### Part 1: Basic Information Setup"""

# Define consolidated paths for unique file counting
train_base_path = os.path.join(dataset_path, 'New Plant Diseases Dataset(Augmented)', 'New Plant Diseases Dataset(Augmented)', 'train')
valid_base_path = os.path.join(dataset_path, 'New Plant Diseases Dataset(Augmented)', 'New Plant Diseases Dataset(Augmented)', 'valid')
test_base_path = os.path.join(dataset_path, 'test', 'test')

unique_image_files = set()

# Function to collect image file paths into a set
def collect_image_files(directory_path, file_set):
    if os.path.exists(directory_path):
        for root, _, files in os.walk(directory_path):
            for file in files:
                # Assuming image files seem to all be .jpg, but just to be safe
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    file_set.add(os.path.join(root, file))
    else:
        print(f"Warning: Directory not found: {directory_path}")

print("Collecting unique image file paths...")

collect_image_files(train_base_path, unique_image_files)
collect_image_files(valid_base_path, unique_image_files)
collect_image_files(test_base_path, unique_image_files)

total_unique_files = len(unique_image_files)

print(f"\n--- Dataset Summary (Unique Files) ---")
print(f"Total unique image files across train, valid, and test datasets: {total_unique_files}")

# Define the base path for the augmented dataset
augmented_dataset_path = actual_data_paths['Augmented Dataset']

data = []

# Helper function to extract plant and disease from a path segment
def parse_plant_disease_from_folder(folder_name):
    # Expected format: 'Plant___Disease'
    if '___' in folder_name:
        plant, disease = folder_name.split('___', 1)
        # Clean up plant name (e.g., 'Corn_(maize)' to 'Corn')
        plant = plant.replace('_(maize)', '').strip()
        disease = disease.replace('_', ' ').strip()
        return plant, disease
    return 'Unknown Plant', folder_name # Fallback if format is not as expected

print("Collecting image metadata from train and valid datasets...")

# Traverse 'train' and 'valid' directories within the augmented dataset
for dataset_subdir in ['train', 'valid']:
    current_path = os.path.join(augmented_dataset_path, dataset_subdir)

    if os.path.exists(current_path):
        for root, _, files in os.walk(current_path):
            # The parent folder of the images (root) contains 'Plant___Disease'
            # Get the last segment of the path which should be 'Plant___Disease'
            folder_segment = os.path.basename(root)
            plant_type, disease_type = parse_plant_disease_from_folder(folder_segment)

            for file in files:
                if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                    data.append({
                        'Image Name': file,
                        'Plant Type': plant_type,
                        'Disease': disease_type,
                        'Dataset Type': dataset_subdir,
                        'Original_Folder_Segment': folder_segment # Add this new column
                    })
    else:
        print(f"Warning: Directory not found: {current_path}")

# Create DataFrame
image_df = pd.DataFrame(data)

# Display the first few rows and summary
print(f"\nTotal images collected: {len(image_df)}")
display(image_df.head())

# Define the test base path from the consolidated actual_data_paths
test_base_path = actual_data_paths['Test Dataset']

# Populate test_files by listing image files in the test directory
test_files = []
if os.path.exists(test_base_path):
    for file_name in os.listdir(test_base_path):
        if os.path.isfile(os.path.join(test_base_path, file_name)) and file_name.lower().endswith(('.png', '.jpg', '.jpeg')):
            test_files.append(file_name)

print(f"Successfully collected {len(test_files)} test files.")
print("First 5 test files:", test_files[:5])

import re
import os

def parse_filename(filename):
    # This function is designed to parse filenames from the test dataset,
    # which seem to follow a pattern like 'PlantDiseaseNumber.JPG'.
    # Examples: 'TomatoEarlyBlight6.JPG', 'PotatoHealthy2.JPG'

    plant_types_mapping = {
        'Tomato': 'Tomato',
        'Potato': 'Potato',
        'Corn': 'Corn',
        'Apple': 'Apple',
        'Soybean': 'Soybean',
        'Grape': 'Grape',
        'Peach': 'Peach',
        'Orange': 'Orange',
        'Strawberry': 'Strawberry',
        'PepperBell': 'Bell Pepper',
        'Cherry': 'Cherry'
    }

    # Remove file extension and any trailing digits from the filename base
    name_part = os.path.splitext(filename)[0]
    name_part_no_digits = re.sub(r'\d+$', '', name_part)

    plant_type = "Unknown Plant"
    disease = "Unknown Disease"

    # Iterate through plant types to find a match at the beginning of the filename part
    # Sort by length in descending order to match longer plant names first (e.g., 'PepperBell' before 'Pepper')
    sorted_plant_keys = sorted(plant_types_mapping.keys(), key=len, reverse=True)

    for plant_key in sorted_plant_keys:
        if name_part_no_digits.startswith(plant_key):
            plant_type = plant_types_mapping[plant_key]
            disease_raw = name_part_no_digits[len(plant_key):]

            # Format disease: insert spaces before capital letters and then title case
            # e.g., 'EarlyBlight' -> 'Early Blight', 'YellowCurlVirus' -> 'Yellow Curl Virus'
            if disease_raw:
                disease = re.sub(r'(?<!^)(?=[A-Z])', ' ', disease_raw).title()
            else:
                # If no disease part is extracted, assume it's healthy for a known plant
                disease = "Healthy"
            break # Found a match, exit loop

    return plant_type, disease


# Collect metadata for the test dataset
test_data = []
for f_name in test_files:
    plant_name, disease_name = parse_filename(f_name)
    test_data.append({
        'Image Name': f_name,
        'Plant Type': plant_name,
        'Disease': disease_name,
        'Dataset Type': 'test',
        'Original_Folder_Segment': 'test' # For test images, the folder segment is simply 'test' or not applicable in the same way
    })

# Create a DataFrame for the test data
test_df = pd.DataFrame(test_data)

# Concatenate the new test_df with the existing image_df
# Reset the index of the combined DataFrame
image_df = pd.concat([image_df, test_df], ignore_index=True)

print(f"\nTotal images in the combined `image_df`: {len(image_df)}")
display(image_df.tail()) # Display the last few rows to show test data

"""### Part 2: Data Cleaning"""

#Basic stats
image_df.describe()

#Get data types
image_df.info()

#Get dataframe shape
image_df.shape

#Check for any missing values
image_df.isnull().sum()

# Display the plant type counts
print("\Plant type counts:")
display(image_df['Plant Type'].value_counts())

# Change 'Pepper,_bell' to 'Bell Pepper'
image_df['Plant Type'] = image_df['Plant Type'].replace('Pepper,_bell', 'Bell Pepper')

# Change 'Cherry_(including_sour)' to 'Cherry'
image_df['Plant Type'] = image_df['Plant Type'].replace('Cherry_(including_sour)', 'Cherry')

# Display the updated plant type counts to confirm the change
print("\nUpdated plant type counts:")
display(image_df['Plant Type'].value_counts())

# Display the disease type counts
print("\nDisease type counts:")
display(image_df['Disease'].value_counts())

print("Disease counts before capitalization:")
display(image_df['Disease'].value_counts().head())

# Capitalize the first letter of each word in the 'Disease' column
image_df['Disease'] = image_df['Disease'].str.title()

print("Disease counts after capitalization:")
display(image_df['Disease'].value_counts().head())

# Change 'spider mites...' to 'Spider Mites' in the 'Disease' column
image_df['Disease'] = image_df['Disease'].replace('Spider Mites Two-Spotted Spider Mite', 'Spider Mites')

# Display the disease type counts
print("\nDisease type counts:")
display(image_df['Disease'].value_counts())

"""### Part 3: Basic Data Visualization"""

# Calculate the counts of each plant type
plant_type_counts = image_df['Plant Type'].value_counts()

# Set a style to look better
sns.set_style("whitegrid")

# Create the bar plot
plt.figure(figsize=(12, 7))

sns.barplot(x=plant_type_counts.index, y=plant_type_counts.values, hue=plant_type_counts.index, palette='viridis', legend=False)

plt.title('Distribution of Images by Plant Type', fontsize=16)
plt.xlabel('Plant Type', fontsize=12)
plt.ylabel('Number of Images', fontsize=12)
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# Calculate the counts of each disease type
disease_counts = image_df['Disease'].value_counts()

# Set a style to look better
sns.set_style("whitegrid")

# Create the bar plot
plt.figure(figsize=(14, 8)) # Adjust figure size for potentially more categories
sns.barplot(x=disease_counts.index, y=disease_counts.values, hue=disease_counts.index, palette='plasma', legend=False)

plt.title('Distribution of Images by Disease Type', fontsize=16)
plt.xlabel('Disease Type', fontsize=12)
plt.ylabel('Number of Images', fontsize=12)
plt.xticks(rotation=90, ha='right') # Rotate labels for better readability, especially if many diseases
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

# Calculate the counts of each disease type
disease_counts = image_df['Disease'].value_counts()

# Set a style to look better
sns.set_style("whitegrid")

# Create the bar plot
plt.figure(figsize=(14, 8)) # Adjust figure size for potentially more categories
sns.barplot(x=disease_counts.index, y=disease_counts.values, hue=disease_counts.index, palette='plasma', legend=False)

plt.title('Distribution of Images by Disease Type (Log Scale)', fontsize=16)
plt.xlabel('Disease Type', fontsize=12)
plt.ylabel('Number of Images (Log Scale)', fontsize=12)
plt.yscale('log') # Apply log transform to the y-axis
plt.xticks(rotation=90, ha='right') # Rotate labels for better readability, especially if many diseases
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.colors as mcolors

# Create a contingency table (crosstab) of Plant Type and Disease
plant_disease_crosstab = pd.crosstab(image_df['Plant Type'], image_df['Disease'])

# Get the original viridis colormap
original_cmap = plt.cm.viridis

# Create a new colormap with white for the lowest values (0)
# Set the first color entry to white
colors = original_cmap(np.linspace(0, 1, 256))
colors[0] = np.array([1, 1, 1, 1])  # RGBA for white
custom_cmap = mcolors.ListedColormap(colors)

# Set a style to look better
sns.set_style("whitegrid")

# Create the heatmap
plt.figure(figsize=(18, 10)) # Adjust figure size for readability
sns.heatmap(plant_disease_crosstab, cmap=custom_cmap, annot=False, fmt='d', linewidths=.5)

plt.title('Heatmap of Image Counts by Plant Type and Disease (0 as White)', fontsize=18)
plt.xlabel('Disease', fontsize=14)
plt.ylabel('Plant Type', fontsize=14)
plt.xticks(rotation=90, ha='right')
plt.yticks(rotation=0)
plt.tight_layout() # Adjust layout to prevent labels from overlapping
plt.show()

"""### Part 4: Setup Train/Test Split"""

train_df = image_df[image_df['Dataset Type'] == 'train'].reset_index(drop=True)
valid_df = image_df[image_df['Dataset Type'] == 'valid'].reset_index(drop=True)
test_df_final = image_df[image_df['Dataset Type'] == 'test'].reset_index(drop=True)

print("--- Train DataFrame ---")
display(train_df.head())
print(f"Shape of train_df: {train_df.shape}\n")

print("--- Validation DataFrame ---")
display(valid_df.head())
print(f"Shape of valid_df: {valid_df.shape}\n")

print("--- Test DataFrame ---")
display(test_df_final.head())
print(f"Shape of test_df_final: {test_df_final.shape}\n")

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import os

def get_image_path(row, base_path):
    # Use the stored original folder segment directly for augmented datasets
    if row['Dataset Type'] in ['train', 'valid']:
        class_folder = row['Original_Folder_Segment']
    else:
        # For test images, they are directly in the test_base_path
        class_folder = '' # No subfolder for test images relative to their base path

    # Construct the full image path
    # If class_folder is empty, os.path.join handles it correctly (i.e., base_path/image_name)
    image_full_path = os.path.join(base_path, class_folder, row['Image Name'])
    return image_full_path

# Get the first 9 images from the training DataFrame
sample_images_df = train_df.head(9)

plt.figure(figsize=(10, 10))
for i, (index, row) in enumerate(sample_images_df.iterrows()):
    plt.subplot(3, 3, i + 1)
    image_path = get_image_path(row, train_base_path)

    try:
        img = mpimg.imread(image_path)
        plt.imshow(img)
        plt.title(f"{row['Plant Type']}\n{row['Disease']}", fontsize=10)
        plt.axis('off')
    except FileNotFoundError:
        print(f"Image not found: {image_path}")
        plt.title(f"Not found:\n{row['Image Name']}", fontsize=10)
        plt.axis('off')
    except Exception as e:
        print(f"Error loading {image_path}: {e}")
        plt.title(f"Error:\n{row['Image Name']}", fontsize=10)
        plt.axis('off')

plt.tight_layout()
plt.show()

IMG_HEIGHT = 224
IMG_WIDTH = 224
BATCH_SIZE = 32

# Retrieve unique class names only from the 'train' and 'valid' datasets
# This ensures the class names correspond to the actual subdirectories in the training and validation paths
class_names = sorted(image_df[image_df['Dataset Type'].isin(['train', 'valid'])]['Original_Folder_Segment'].unique())

# Create training dataset using image_dataset_from_directory
train_ds = keras.utils.image_dataset_from_directory(
    train_base_path, # Path to the main directory of your training images
    labels='inferred',
    label_mode='categorical', # Use 'categorical' for one-hot encoding labels
    class_names=class_names,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    interpolation='nearest',
    batch_size=BATCH_SIZE,
    shuffle=True
)

# Create validation dataset using image_dataset_from_directory
valid_ds = keras.utils.image_dataset_from_directory(
    valid_base_path, # Path to the main directory of your validation images
    labels='inferred',
    label_mode='categorical',
    class_names=class_names,
    image_size=(IMG_HEIGHT, IMG_WIDTH),
    interpolation='nearest',
    batch_size=BATCH_SIZE,
    shuffle=False
)

# Print the number of batches in each dataset and a sample batch
print(f"Number of training batches: {len(train_ds)}")
print(f"Number of validation batches: {len(valid_ds)}")

for image_batch, label_batch in train_ds.take(1):
    print("Image batch shape:", image_batch.shape)
    print("Label batch shape:", label_batch.shape)
    print("Class names (first 10):", class_names[:10])

"""https://keras.io/examples/vision/image_classification_from_scratch/

I was looking at this for reference, looks like augmentation might be something I could do for the smaller datasets.

### Part 5: Image Preprocessing and Augmentation
"""

# Rescale from [0,255] to [0,1]
rescale = keras.Sequential([
    layers.Rescaling(1./255)
])

# Data augmentation for transformations (i just did it for all randomly)
data_augmentation = keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2),
    layers.RandomZoom(0.2),
])

# Add to dataset

AUTOTUNE = tf_data.AUTOTUNE

def prepare_dataset(ds, shuffle=False, augment=False):
    # Rescale all datasets
    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)

    if shuffle:
        ds = ds.shuffle(1000) # Shuffle only the training data

    # Apply data augmentation to the training dataset
    if augment:
        ds = ds.map(lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=AUTOTUNE)

    # Cache the dataset elements in memory or disk
    ds = ds.cache()

    # Prefetch elements to optimize pipeline performance
    ds = ds.prefetch(buffer_size=AUTOTUNE)

    return ds

train_ds = prepare_dataset(train_ds, shuffle=True, augment=True)
valid_ds = prepare_dataset(valid_ds)

print("Datasets preprocessed and optimized:")
print(f"  Training dataset: {train_ds}")
print(f"  Validation dataset: {valid_ds}")

"""### Part 6: Build and Train the CNN Model"""

# Define CNN
num_classes = len(class_names)

model = keras.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(num_classes, activation='softmax')
])

model.summary()

# Compile model with Adam optimizer and categorical cross entropy while monitoring accuracy
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

epochs = 10 # You can adjust the number of epochs

history = model.fit(
    train_ds,
    validation_data=valid_ds,
    epochs=epochs
)